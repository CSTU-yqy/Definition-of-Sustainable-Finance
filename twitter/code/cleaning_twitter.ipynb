{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/qianyuyang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/qianyuyang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/qianyuyang/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/qianyuyang/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "#necessary tools\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "import numpy as np\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening file, please change directory\n",
    "twitter = pd.read_csv('../data/twitter_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>time</th>\n",
       "      <th>content</th>\n",
       "      <th>search_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2023-06-01T14:04:44.000Z</td>\n",
       "      <td>\" #EuropeanCommission  Responds to ESA's Quest...</td>\n",
       "      <td>\"sustainablefinance\"AND\"definition\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-06-01T14:04:44.000Z</td>\n",
       "      <td>Sie werden keinen normalen Rucksack mehr trage...</td>\n",
       "      <td>\"sustainablefinance\"AND\"definition\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-06-04T22:00:27.000Z</td>\n",
       "      <td>Definitions of key sustainable investment conc...</td>\n",
       "      <td>\"sustainablefinance\"AND\"definition\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-06-12T20:00:21.000Z</td>\n",
       "      <td>link. \\n\\nReal estate groups are calling for c...</td>\n",
       "      <td>\"sustainablefinance\"AND\"definition\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2023-06-01T10:30:52.000Z</td>\n",
       "      <td>Common definition of  #greenwashing  published...</td>\n",
       "      <td>\"sustainablefinance\"AND\"definition\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                      time  \\\n",
       "0           0  2023-06-01T14:04:44.000Z   \n",
       "1           1  2023-06-01T14:04:44.000Z   \n",
       "2           2  2023-06-04T22:00:27.000Z   \n",
       "3           3  2023-06-12T20:00:21.000Z   \n",
       "4           4  2023-06-01T10:30:52.000Z   \n",
       "\n",
       "                                             content  \\\n",
       "0  \" #EuropeanCommission  Responds to ESA's Quest...   \n",
       "1  Sie werden keinen normalen Rucksack mehr trage...   \n",
       "2  Definitions of key sustainable investment conc...   \n",
       "3  link. \\n\\nReal estate groups are calling for c...   \n",
       "4  Common definition of  #greenwashing  published...   \n",
       "\n",
       "                            search_key  \n",
       "0  \"sustainablefinance\"AND\"definition\"  \n",
       "1  \"sustainablefinance\"AND\"definition\"  \n",
       "2  \"sustainablefinance\"AND\"definition\"  \n",
       "3  \"sustainablefinance\"AND\"definition\"  \n",
       "4  \"sustainablefinance\"AND\"definition\"  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating stopwords\n",
    "stop_words_2 = set(stopwords.words('English'))\n",
    "ps = PorterStemmer()\n",
    "wn = WordNetLemmatizer()\n",
    "punctuation_set = set(string.punctuation)\n",
    "customized_stopwords = set(['hi','hello','hey'])\n",
    "stop_words = stop_words_2 | punctuation_set | customized_stopwords\n",
    "#storing stop words list. Uncomment the next line if you need it.\n",
    "#pd.Series(list(stop_words)).to_csv('./stop_words.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input: treebank_tag: a word\n",
    "#output: a woednet object, will be used later in lemmatizing\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input: sentence: a sentence, usually an str; lower: bool. set it True if you wish to lower the sentence.\n",
    "#output: a list. Each element inside is a tuple containing a word and its part of speech.\n",
    "\n",
    "def pos_tagging(sentence,lower = True):\n",
    "    if lower:\n",
    "        tokens = nltk.word_tokenize(sentence.lower())  # tokenizing\n",
    "    else:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "    tagged = nltk.pos_tag(tokens)  # part-of-speech tagging\n",
    "    return tagged\n",
    "\n",
    "\n",
    "#there could be some words that are not recognized. In this case their part-of-speech would be unknown or ''(empty).\n",
    "#These tags need to be standardized\n",
    "def replace_unknown_pos(tagged):\n",
    "    for i, (word, pos) in enumerate(tagged):\n",
    "        if pos == '' or pos == 'unknown':\n",
    "            tagged[i] = (word, 'unknown')\n",
    "    return tagged\n",
    "\n",
    "# an example. uncomment the next 3 lines if you want to see it.\n",
    "#sentence = \"This is a sample sentence.\"\n",
    "#tagged_words = pos_tagging(sentence)\n",
    "#tagged_words = replace_unknown_pos(tagged_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknowns = []\n",
    "#input: sentence: a sentence, usually an str; noun_only: bool, set it True if you wish only work with nouns in the sencence.\n",
    "#output: lemmatized and tokenized and filtered(if noun_only == True) sentences, stored in a list.\n",
    "#the list unknowns is global, storing all unknown words. You can choose to add them back or not in later analysis.\n",
    "def wash(sentence : str,noun_only = True):\n",
    "    global unknowns\n",
    "    if not pd.isna(sentence):\n",
    "        pos_tags = pos_tagging(sentence)\n",
    "        pos_tags = replace_unknown_pos(pos_tags)\n",
    "        unknown_s =[word for word,pos in pos_tags if pos == 'unknown'] \n",
    "        unknowns = unknown_s + unknowns\n",
    "        known_s = [(word,pos) for word,pos in pos_tags if not pos == 'unknown']\n",
    "\n",
    "        #If we are working with nouns or not.\n",
    "        if noun_only:\n",
    "            noun = [(word,pos) for word,pos in known_s if pos.startswith('N')]\n",
    "            lemmatized_sentence = [wn.lemmatize(w,pos = get_wordnet_pos(pos_tag)) for w,pos_tag in noun]\n",
    "        else:\n",
    "            lemmatized_sentence = [wn.lemmatize(w,pos = get_wordnet_pos(pos_tag)) for w,pos_tag in known_s]\n",
    "        filtered_sentence =  [w for w in lemmatized_sentence if not w in stop_words]\n",
    "        return filtered_sentence\n",
    "    return pd.NA\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = twitter.reset_index().drop(columns = ['index'])\n",
    "#drop index numbers and reset a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-b0fc9aeb61db>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  twitter.time[189] = '2013-05-30'\n"
     ]
    }
   ],
   "source": [
    "#extracting tag\n",
    "twitter['hash_tags'] = twitter['content'].str.extract(r'#(\\w+)\\s')\n",
    "#Changeing time format\n",
    "#at position 189 there is a date stored in a different format, need to change it.\n",
    "twitter.time[189] = '2013-05-30'\n",
    "twitter['datetime'] = pd.to_datetime(twitter['time'])\n",
    "twitter['date'] = [i.strftime('%Y-%m-%d') for i in twitter.datetime]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_twitter = twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing and lemmatizing sentences\n",
    "new_twitter['content_all'] = [wash(i,False) for i in twitter['content']]\n",
    "new_twitter['content_noun'] = [wash(i,noun_only = True) for i in twitter['content']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>time</th>\n",
       "      <th>content</th>\n",
       "      <th>search_key</th>\n",
       "      <th>hash_tags</th>\n",
       "      <th>datetime</th>\n",
       "      <th>date</th>\n",
       "      <th>content_all</th>\n",
       "      <th>content_noun</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2023-06-01T14:04:44.000Z</td>\n",
       "      <td>\" #EuropeanCommission  Responds to ESA's Quest...</td>\n",
       "      <td>\"sustainablefinance\"AND\"definition\"</td>\n",
       "      <td>EuropeanCommission</td>\n",
       "      <td>2023-06-01 14:04:44+00:00</td>\n",
       "      <td>2023-06-01</td>\n",
       "      <td>[``, europeancommission, respond, esa, 's, que...</td>\n",
       "      <td>[europeancommission, question, interpretation,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2023-06-01T14:04:44.000Z</td>\n",
       "      <td>Sie werden keinen normalen Rucksack mehr trage...</td>\n",
       "      <td>\"sustainablefinance\"AND\"definition\"</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2023-06-01 14:04:44+00:00</td>\n",
       "      <td>2023-06-01</td>\n",
       "      <td>[sie, werden, keinen, normalen, rucksack, mehr...</td>\n",
       "      <td>[sie, keinen, rucksack, mehr, tragen, liebe, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2023-06-04T22:00:27.000Z</td>\n",
       "      <td>Definitions of key sustainable investment conc...</td>\n",
       "      <td>\"sustainablefinance\"AND\"definition\"</td>\n",
       "      <td>sustainablefinance</td>\n",
       "      <td>2023-06-04 22:00:27+00:00</td>\n",
       "      <td>2023-06-04</td>\n",
       "      <td>[definition, key, sustainable, investment, con...</td>\n",
       "      <td>[definition, investment, concept, report, fram...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2023-06-12T20:00:21.000Z</td>\n",
       "      <td>link. \\n\\nReal estate groups are calling for c...</td>\n",
       "      <td>\"sustainablefinance\"AND\"definition\"</td>\n",
       "      <td>ESG</td>\n",
       "      <td>2023-06-12 20:00:21+00:00</td>\n",
       "      <td>2023-06-12</td>\n",
       "      <td>[link, real, estate, group, call, change, 'con...</td>\n",
       "      <td>[link, estate, group, change, sfdr, rule, defi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2023-06-01T10:30:52.000Z</td>\n",
       "      <td>Common definition of  #greenwashing  published...</td>\n",
       "      <td>\"sustainablefinance\"AND\"definition\"</td>\n",
       "      <td>greenwashing</td>\n",
       "      <td>2023-06-01 10:30:52+00:00</td>\n",
       "      <td>2023-06-01</td>\n",
       "      <td>[common, definition, greenwashing, publish, es...</td>\n",
       "      <td>[definition, esas, sustainablefinance, eutaxon...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                      time  \\\n",
       "0           0  2023-06-01T14:04:44.000Z   \n",
       "1           1  2023-06-01T14:04:44.000Z   \n",
       "2           2  2023-06-04T22:00:27.000Z   \n",
       "3           3  2023-06-12T20:00:21.000Z   \n",
       "4           4  2023-06-01T10:30:52.000Z   \n",
       "\n",
       "                                             content  \\\n",
       "0  \" #EuropeanCommission  Responds to ESA's Quest...   \n",
       "1  Sie werden keinen normalen Rucksack mehr trage...   \n",
       "2  Definitions of key sustainable investment conc...   \n",
       "3  link. \\n\\nReal estate groups are calling for c...   \n",
       "4  Common definition of  #greenwashing  published...   \n",
       "\n",
       "                            search_key           hash_tags  \\\n",
       "0  \"sustainablefinance\"AND\"definition\"  EuropeanCommission   \n",
       "1  \"sustainablefinance\"AND\"definition\"                 NaN   \n",
       "2  \"sustainablefinance\"AND\"definition\"  sustainablefinance   \n",
       "3  \"sustainablefinance\"AND\"definition\"                 ESG   \n",
       "4  \"sustainablefinance\"AND\"definition\"        greenwashing   \n",
       "\n",
       "                    datetime        date  \\\n",
       "0  2023-06-01 14:04:44+00:00  2023-06-01   \n",
       "1  2023-06-01 14:04:44+00:00  2023-06-01   \n",
       "2  2023-06-04 22:00:27+00:00  2023-06-04   \n",
       "3  2023-06-12 20:00:21+00:00  2023-06-12   \n",
       "4  2023-06-01 10:30:52+00:00  2023-06-01   \n",
       "\n",
       "                                         content_all  \\\n",
       "0  [``, europeancommission, respond, esa, 's, que...   \n",
       "1  [sie, werden, keinen, normalen, rucksack, mehr...   \n",
       "2  [definition, key, sustainable, investment, con...   \n",
       "3  [link, real, estate, group, call, change, 'con...   \n",
       "4  [common, definition, greenwashing, publish, es...   \n",
       "\n",
       "                                        content_noun  \n",
       "0  [europeancommission, question, interpretation,...  \n",
       "1  [sie, keinen, rucksack, mehr, tragen, liebe, d...  \n",
       "2  [definition, investment, concept, report, fram...  \n",
       "3  [link, estate, group, change, sfdr, rule, defi...  \n",
       "4  [definition, esas, sustainablefinance, eutaxon...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_twitter.head()\n",
    "#our washed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment the next line if you wish to store it.\n",
    "#new_twitter.to_csv('../data/new_twitter.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
