{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/qianyuyang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/qianyuyang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/qianyuyang/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/qianyuyang/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "#necessary tools\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "import numpy as np\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_original</th>\n",
       "      <th>content_original</th>\n",
       "      <th>comment_original</th>\n",
       "      <th>time</th>\n",
       "      <th>accurate_time</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Think Like a Climate Investor #1 [Series]</td>\n",
       "      <td>Good investors (us) should always be modeling ...</td>\n",
       "      <td>Does this understanding of additionality accou...</td>\n",
       "      <td>1 day ago</td>\n",
       "      <td>['Fri, May 05, 2023, 04:14:08 PM Central Europ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Scan the globe for the best stocks to invest i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Promoted</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the best certificate/masters out there...</td>\n",
       "      <td>I currently work in the sustainability finance...</td>\n",
       "      <td>\\nWhat’s your job now?\\nI work in finance, par...</td>\n",
       "      <td>1 day ago</td>\n",
       "      <td>['Fri, May 05, 2023, 11:37:52 AM Central Europ...</td>\n",
       "      <td>Discussion / Question</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Congrats to those that passed the CESGA March ...</td>\n",
       "      <td>CESGA just announced the results for the CESGA...</td>\n",
       "      <td>I have no idea how I passed it, but it was a g...</td>\n",
       "      <td>4 days ago</td>\n",
       "      <td>['Tue, May 02, 2023, 04:45:13 PM Central Europ...</td>\n",
       "      <td>Careers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Solar and Wind PPA Prices Surge in North Ameri...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5 days ago</td>\n",
       "      <td>['Mon, May 01, 2023, 06:44:27 PM Central Europ...</td>\n",
       "      <td>Markets</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      title_original  \\\n",
       "0          Think Like a Climate Investor #1 [Series]   \n",
       "1  Scan the globe for the best stocks to invest i...   \n",
       "2  What is the best certificate/masters out there...   \n",
       "3  Congrats to those that passed the CESGA March ...   \n",
       "4  Solar and Wind PPA Prices Surge in North Ameri...   \n",
       "\n",
       "                                    content_original  \\\n",
       "0  Good investors (us) should always be modeling ...   \n",
       "1                                                NaN   \n",
       "2  I currently work in the sustainability finance...   \n",
       "3  CESGA just announced the results for the CESGA...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                    comment_original        time  \\\n",
       "0  Does this understanding of additionality accou...   1 day ago   \n",
       "1                                                NaN    Promoted   \n",
       "2  \\nWhat’s your job now?\\nI work in finance, par...   1 day ago   \n",
       "3  I have no idea how I passed it, but it was a g...  4 days ago   \n",
       "4                                                NaN  5 days ago   \n",
       "\n",
       "                                       accurate_time                  label  \n",
       "0  ['Fri, May 05, 2023, 04:14:08 PM Central Europ...                    NaN  \n",
       "1                                                NaN                    NaN  \n",
       "2  ['Fri, May 05, 2023, 11:37:52 AM Central Europ...  Discussion / Question  \n",
       "3  ['Tue, May 02, 2023, 04:45:13 PM Central Europ...                Careers  \n",
       "4  ['Mon, May 01, 2023, 06:44:27 PM Central Europ...                Markets  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hot = pd.read_csv('../data/Reddit_raw.csv')\n",
    "#reading raw data. Remember to change your directory\n",
    "hot = hot.rename(columns={\n",
    "    'title' : 'title_original',\n",
    "    'content': 'content_original',\n",
    "    'comment': 'comment_original'\n",
    "})\n",
    "hot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating stop words\n",
    "stop_words_2 = set(stopwords.words('English'))\n",
    "ps = PorterStemmer()\n",
    "wn = WordNetLemmatizer()\n",
    "punctuation_set = set(string.punctuation)\n",
    "customized_stopwords = set(['hi','hello','hey'])\n",
    "stop_words = stop_words_2 | punctuation_set | customized_stopwords\n",
    "#storing stop words list. Uncomment if you need it.\n",
    "#pd.Series(list(stop_words)).to_csv('./stop_words.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input: treebank_tag: a word\n",
    "#output: a woednet object, will be used later in lemmatizing\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  \n",
    "    \n",
    "#input: sentence: a sentence, usually an str; lower: bool. set it True if you wish to lower the sentence.\n",
    "#output: a list. Each element inside is a tuple containing a word and its part of speech.\n",
    "\n",
    "def pos_tagging(sentence,lower = True):\n",
    "    if lower:\n",
    "        tokens = nltk.word_tokenize(sentence.lower())  # tokenizing\n",
    "    else:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "    tagged = nltk.pos_tag(tokens)  # part-of-speech tagging\n",
    "    return tagged\n",
    "\n",
    "#there could be some words that are not recognized. In this case their part-of-speech would be unknown or ''(empty).\n",
    "#These tags need to be standardized\n",
    "def replace_unknown_pos(tagged):\n",
    "    for i, (word, pos) in enumerate(tagged):\n",
    "        if pos == '' or pos == 'unknown':\n",
    "            tagged[i] = (word, 'unknown')\n",
    "    return tagged\n",
    "\n",
    "\n",
    "# an example. uncomment the next 3 lines if you want to see it.\n",
    "#sentence = \"This is a sample sentence.\"\n",
    "#tagged_words = pos_tagging(sentence)\n",
    "#tagged_words = replace_unknown_pos(tagged_words)\n",
    "\n",
    "unknowns = []\n",
    "#input: sentence: a sentence, usually an str; noun_only: bool, set it True if you wish only work with nouns in the sencence.\n",
    "#output: lemmatized and tokenized and filtered(if noun_only == True) sentences, stored in a list.\n",
    "#the list unknowns is global, storing all unknown words. You can choose to add them back or not in later analysis.\n",
    "def wash(sentence : str,is_noun = True):\n",
    "    global unknowns\n",
    "    if not pd.isna(sentence):\n",
    "        pos_tags = pos_tagging(sentence)\n",
    "        pos_tags = replace_unknown_pos(pos_tags)\n",
    "        unknown_s =[word for word,pos in pos_tags if pos == 'unknown'] \n",
    "        unknowns = unknown_s + unknowns\n",
    "        known_s = [(word,pos) for word,pos in pos_tags if not pos == 'unknown']\n",
    "\n",
    "        #If we are working with nouns or not.\n",
    "        noun = [(word,pos) for word,pos in known_s if pos.startswith('N')]\n",
    "        if is_noun:\n",
    "            lemmatized_sentence = [wn.lemmatize(w,pos = get_wordnet_pos(pos_tag)) for w,pos_tag in noun]\n",
    "        else:\n",
    "            lemmatized_sentence = [wn.lemmatize(w,pos = get_wordnet_pos(pos_tag)) for w,pos_tag in known_s]\n",
    "        filtered_sentence =  [w for w in lemmatized_sentence if not w in stop_words]\n",
    "        return filtered_sentence\n",
    "    return pd.NA\n",
    "    \n",
    "def convert_time(date_str : str):\n",
    "    pattern = r'\\w+, \\w+ \\d+, \\d{4}, \\d{2}:\\d{2}:\\d{2} [AP]M'\n",
    "    empty_time = '[]'\n",
    "    if not pd.isna(date_str):\n",
    "        if not date_str == empty_time:\n",
    "            match = re.search(pattern, date_str)\n",
    "            if match:\n",
    "                extracted_date = match.group()\n",
    "                datetime_obj = datetime.strptime(extracted_date, '%a, %b %d, %Y, %I:%M:%S %p')\n",
    "                return datetime_obj\n",
    "            else:\n",
    "                return date_str\n",
    "        else: return pd.NA\n",
    "    else:\n",
    "        return pd.NA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_hot = pd.DataFrame()\n",
    "unknowns = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatizing all words\n",
    "def convert_dataframe(hot, is_noun = True):\n",
    "    new_hot = pd.DataFrame()\n",
    "    tag = '_noun' if is_noun else '_all'\n",
    "    for c in hot.columns:\n",
    "        if not c in ['label','accurate_time']:\n",
    "            parts = c.split('_')\n",
    "            new_name = parts[0] + tag\n",
    "            new_hot[new_name] = hot[c].apply(wash,is_noun = is_noun)\n",
    "        else: new_hot['datetime'] = hot['accurate_time'].apply(convert_time) #converting time\n",
    "    new_hot['date'] = [pd.NA if pd.isna(t) else t.strftime(\"%Y-%m-%d\") for t in new_hot['datetime']]\n",
    "    return new_hot\n",
    "\n",
    "new_hot_noun = convert_dataframe(hot,True)\n",
    "new_hot_all = convert_dataframe(hot,False)\n",
    "#putting all three dataframe together.\n",
    "concatenated_df =  pd.concat([hot,new_hot_noun,new_hot_all],axis = 1)\n",
    "concatenated_df = concatenated_df.loc[:, ~concatenated_df.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title_original</th>\n",
       "      <th>content_original</th>\n",
       "      <th>comment_original</th>\n",
       "      <th>time</th>\n",
       "      <th>accurate_time</th>\n",
       "      <th>label</th>\n",
       "      <th>title_noun</th>\n",
       "      <th>content_noun</th>\n",
       "      <th>comment_noun</th>\n",
       "      <th>title_all</th>\n",
       "      <th>content_all</th>\n",
       "      <th>comment_all</th>\n",
       "      <th>datetime</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Think Like a Climate Investor #1 [Series]</td>\n",
       "      <td>Good investors (us) should always be modeling ...</td>\n",
       "      <td>Does this understanding of additionality accou...</td>\n",
       "      <td>1 day ago</td>\n",
       "      <td>['Fri, May 05, 2023, 04:14:08 PM Central Europ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[climate, investor, series]</td>\n",
       "      <td>[investor, future, section, ira, incentive, hy...</td>\n",
       "      <td>[understanding, additionality, account, renewa...</td>\n",
       "      <td>[think, like, climate, investor, 1, series]</td>\n",
       "      <td>[good, investor, u, always, model, possible, f...</td>\n",
       "      <td>[understanding, additionality, account, renewa...</td>\n",
       "      <td>2023-05-05 16:14:08</td>\n",
       "      <td>2023-05-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Scan the globe for the best stocks to invest i...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Promoted</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[globe, stock, globalanalyst, capital, risk]</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>[scan, globe, best, stock, invest, globalanaly...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the best certificate/masters out there...</td>\n",
       "      <td>I currently work in the sustainability finance...</td>\n",
       "      <td>\\nWhat’s your job now?\\nI work in finance, par...</td>\n",
       "      <td>1 day ago</td>\n",
       "      <td>['Fri, May 05, 2023, 11:37:52 AM Central Europ...</td>\n",
       "      <td>Discussion / Question</td>\n",
       "      <td>[certificate/masters, credential, finance, car...</td>\n",
       "      <td>[sustainability, finance, industry, cert/maste...</td>\n",
       "      <td>[job, work, finance, project, finance, develop...</td>\n",
       "      <td>[best, certificate/masters, give, credential, ...</td>\n",
       "      <td>[currently, work, sustainability, finance, ind...</td>\n",
       "      <td>[’, job, work, finance, particularly, project,...</td>\n",
       "      <td>2023-05-05 11:37:52</td>\n",
       "      <td>2023-05-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Congrats to those that passed the CESGA March ...</td>\n",
       "      <td>CESGA just announced the results for the CESGA...</td>\n",
       "      <td>I have no idea how I passed it, but it was a g...</td>\n",
       "      <td>4 days ago</td>\n",
       "      <td>['Tue, May 02, 2023, 04:45:13 PM Central Europ...</td>\n",
       "      <td>Careers</td>\n",
       "      <td>[congrats, cesga, march, exam]</td>\n",
       "      <td>[cesga, result, cesga, exam, march, congrats]</td>\n",
       "      <td>[idea, relief, heard, designation, curriculum,...</td>\n",
       "      <td>[congrats, pass, cesga, march, exam]</td>\n",
       "      <td>[cesga, announce, result, cesga, exam, march, ...</td>\n",
       "      <td>[idea, pass, great, relief, haha, ’, never, he...</td>\n",
       "      <td>2023-05-02 16:45:13</td>\n",
       "      <td>2023-05-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Solar and Wind PPA Prices Surge in North Ameri...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5 days ago</td>\n",
       "      <td>['Mon, May 01, 2023, 06:44:27 PM Central Europ...</td>\n",
       "      <td>Markets</td>\n",
       "      <td>[ppa, price, america, europe, see, price, drop...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>[solar, wind, ppa, price, surge, north, americ...</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>2023-05-01 18:44:27</td>\n",
       "      <td>2023-05-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      title_original  \\\n",
       "0          Think Like a Climate Investor #1 [Series]   \n",
       "1  Scan the globe for the best stocks to invest i...   \n",
       "2  What is the best certificate/masters out there...   \n",
       "3  Congrats to those that passed the CESGA March ...   \n",
       "4  Solar and Wind PPA Prices Surge in North Ameri...   \n",
       "\n",
       "                                    content_original  \\\n",
       "0  Good investors (us) should always be modeling ...   \n",
       "1                                                NaN   \n",
       "2  I currently work in the sustainability finance...   \n",
       "3  CESGA just announced the results for the CESGA...   \n",
       "4                                                NaN   \n",
       "\n",
       "                                    comment_original        time  \\\n",
       "0  Does this understanding of additionality accou...   1 day ago   \n",
       "1                                                NaN    Promoted   \n",
       "2  \\nWhat’s your job now?\\nI work in finance, par...   1 day ago   \n",
       "3  I have no idea how I passed it, but it was a g...  4 days ago   \n",
       "4                                                NaN  5 days ago   \n",
       "\n",
       "                                       accurate_time                  label  \\\n",
       "0  ['Fri, May 05, 2023, 04:14:08 PM Central Europ...                    NaN   \n",
       "1                                                NaN                    NaN   \n",
       "2  ['Fri, May 05, 2023, 11:37:52 AM Central Europ...  Discussion / Question   \n",
       "3  ['Tue, May 02, 2023, 04:45:13 PM Central Europ...                Careers   \n",
       "4  ['Mon, May 01, 2023, 06:44:27 PM Central Europ...                Markets   \n",
       "\n",
       "                                          title_noun  \\\n",
       "0                        [climate, investor, series]   \n",
       "1       [globe, stock, globalanalyst, capital, risk]   \n",
       "2  [certificate/masters, credential, finance, car...   \n",
       "3                     [congrats, cesga, march, exam]   \n",
       "4  [ppa, price, america, europe, see, price, drop...   \n",
       "\n",
       "                                        content_noun  \\\n",
       "0  [investor, future, section, ira, incentive, hy...   \n",
       "1                                               <NA>   \n",
       "2  [sustainability, finance, industry, cert/maste...   \n",
       "3      [cesga, result, cesga, exam, march, congrats]   \n",
       "4                                               <NA>   \n",
       "\n",
       "                                        comment_noun  \\\n",
       "0  [understanding, additionality, account, renewa...   \n",
       "1                                               <NA>   \n",
       "2  [job, work, finance, project, finance, develop...   \n",
       "3  [idea, relief, heard, designation, curriculum,...   \n",
       "4                                               <NA>   \n",
       "\n",
       "                                           title_all  \\\n",
       "0        [think, like, climate, investor, 1, series]   \n",
       "1  [scan, globe, best, stock, invest, globalanaly...   \n",
       "2  [best, certificate/masters, give, credential, ...   \n",
       "3               [congrats, pass, cesga, march, exam]   \n",
       "4  [solar, wind, ppa, price, surge, north, americ...   \n",
       "\n",
       "                                         content_all  \\\n",
       "0  [good, investor, u, always, model, possible, f...   \n",
       "1                                               <NA>   \n",
       "2  [currently, work, sustainability, finance, ind...   \n",
       "3  [cesga, announce, result, cesga, exam, march, ...   \n",
       "4                                               <NA>   \n",
       "\n",
       "                                         comment_all             datetime  \\\n",
       "0  [understanding, additionality, account, renewa...  2023-05-05 16:14:08   \n",
       "1                                               <NA>                 <NA>   \n",
       "2  [’, job, work, finance, particularly, project,...  2023-05-05 11:37:52   \n",
       "3  [idea, pass, great, relief, haha, ’, never, he...  2023-05-02 16:45:13   \n",
       "4                                               <NA>  2023-05-01 18:44:27   \n",
       "\n",
       "         date  \n",
       "0  2023-05-05  \n",
       "1        <NA>  \n",
       "2  2023-05-05  \n",
       "3  2023-05-02  \n",
       "4  2023-05-01  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reset the order of columns\n",
    "reddit = concatenated_df.reindex(columns= [\n",
    "    'title_original', 'content_original', 'comment_original','time','accurate_time', 'label', 'title_noun', 'content_noun', 'comment_noun', 'title_all', 'content_all', 'comment_all','datetime', 'date'\n",
    "    ])\n",
    "reddit.head()\n",
    "#storing the data. Uncomment if you need it.\n",
    "#reddit.to_csv('../data//Reddit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
